# @package _global_

framework:
  cfg:
    optimizer: torch_optimizer.AdaBelief
    optimizer_kwargs:
      lr: ${settings.optimizer.lr}
      betas: [0.9, 0.999]
      eps: 1e-8
      weight_decay: 0

      amsgrad: False
      weight_decouple: False
      fixed_decay: False
      rectify: False
